{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üéÆ Demo: Trained RL Agent\n",
                "\n",
                "This notebook demonstrates the trained PPO agent navigating in BabyAI environments."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "import numpy as np\n",
                "import minigrid\n",
                "import matplotlib.pyplot as plt\n",
                "from IPython.display import display, clear_output\n",
                "import time"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Environment matching training\n",
                "class MiniGridFlatEnv(gym.Env):\n",
                "    def __init__(self, config=None):\n",
                "        super().__init__()\n",
                "        config = config or {}\n",
                "        env_name = config.get(\"env_name\", \"BabyAI-GoToObj-v0\")\n",
                "        max_steps = config.get(\"max_steps\", 64)\n",
                "        self.env = gym.make(env_name, render_mode=\"rgb_array\")\n",
                "        self.env.unwrapped.max_steps = max_steps\n",
                "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(151,), dtype=np.float32)\n",
                "        self.action_space = self.env.action_space\n",
                "        self.instruction = \"\"\n",
                "    \n",
                "    def _flatten_obs(self, obs):\n",
                "        image = obs[\"image\"].flatten().astype(np.float32) / 10.0\n",
                "        direction = np.zeros(4, dtype=np.float32)\n",
                "        direction[obs[\"direction\"]] = 1.0\n",
                "        return np.concatenate([image, direction])\n",
                "    \n",
                "    def reset(self, *, seed=None, options=None):\n",
                "        obs, info = self.env.reset(seed=seed, options=options)\n",
                "        self.instruction = self.env.unwrapped.mission\n",
                "        return self._flatten_obs(obs), info\n",
                "    \n",
                "    def step(self, action):\n",
                "        obs, reward, term, trunc, info = self.env.step(action)\n",
                "        return self._flatten_obs(obs), reward, term, trunc, info\n",
                "    \n",
                "    def render(self):\n",
                "        return self.env.render()\n",
                "\n",
                "print(\"Environment ready!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Load trained model\n",
                "import ray\n",
                "from ray.rllib.algorithms.ppo import PPOConfig\n",
                "from ray.tune.registry import register_env\n",
                "\n",
                "if ray.is_initialized():\n",
                "    ray.shutdown()\n",
                "ray.init(ignore_reinit_error=True, num_cpus=2)\n",
                "\n",
                "def env_creator(config):\n",
                "    return MiniGridFlatEnv(config)\n",
                "\n",
                "register_env(\"MiniGridFlat-v0\", env_creator)\n",
                "\n",
                "config = (\n",
                "    PPOConfig()\n",
                "    .api_stack(\n",
                "        enable_rl_module_and_learner=False,\n",
                "        enable_env_runner_and_connector_v2=False,\n",
                "    )\n",
                "    .environment(\n",
                "        env=\"MiniGridFlat-v0\",\n",
                "        env_config={\"env_name\": \"BabyAI-GoToObj-v0\", \"max_steps\": 64},\n",
                "    )\n",
                "    .framework(\"torch\")\n",
                "    .env_runners(num_env_runners=0)\n",
                "    .resources(num_gpus=0)\n",
                ")\n",
                "\n",
                "algo = config.build()\n",
                "\n",
                "# CHANGE THIS PATH to your checkpoint\n",
                "CHECKPOINT_PATH = \"../experiments/checkpoints/final\"  # or wherever you extracted it\n",
                "\n",
                "try:\n",
                "    algo.restore(CHECKPOINT_PATH)\n",
                "    print(f\"‚úÖ Loaded model from {CHECKPOINT_PATH}\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è Could not load checkpoint. Using untrained model for demo.\")\n",
                "    print(\"To load your trained model:\")\n",
                "    print(\"1. Unzip trained_model.zip to experiments/\")\n",
                "    print(\"2. Update CHECKPOINT_PATH above\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Visualize agent solving a task\n",
                "ACTION_NAMES = [\"left\", \"right\", \"forward\", \"pickup\", \"drop\", \"toggle\", \"done\"]\n",
                "\n",
                "def run_episode(seed=42, show_steps=True):\n",
                "    env = MiniGridFlatEnv({\"env_name\": \"BabyAI-GoToObj-v0\", \"max_steps\": 64})\n",
                "    obs, _ = env.reset(seed=seed)\n",
                "    \n",
                "    print(f\"üéØ Instruction: {env.instruction}\")\n",
                "    print(\"=\"*50)\n",
                "    \n",
                "    frames = [env.render()]\n",
                "    actions = []\n",
                "    done = False\n",
                "    step = 0\n",
                "    \n",
                "    while not done:\n",
                "        action = algo.compute_single_action(obs)\n",
                "        actions.append(ACTION_NAMES[action])\n",
                "        obs, reward, term, trunc, _ = env.step(action)\n",
                "        done = term or trunc\n",
                "        step += 1\n",
                "        frames.append(env.render())\n",
                "        \n",
                "        if show_steps:\n",
                "            print(f\"Step {step}: {ACTION_NAMES[action]}\")\n",
                "    \n",
                "    if term and reward > 0:\n",
                "        print(f\"\\nüéâ SUCCESS in {step} steps!\")\n",
                "    else:\n",
                "        print(f\"\\n‚ùå Failed after {step} steps\")\n",
                "    \n",
                "    return frames, actions\n",
                "\n",
                "frames, actions = run_episode(seed=42)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Show the trajectory\n",
                "num_frames = min(len(frames), 12)\n",
                "fig, axes = plt.subplots(2, 6, figsize=(15, 5))\n",
                "\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    if i < num_frames:\n",
                "        ax.imshow(frames[i])\n",
                "        if i < len(actions):\n",
                "            ax.set_title(f\"Step {i}: {actions[i]}\")\n",
                "        else:\n",
                "            ax.set_title(\"Done\")\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Evaluate on multiple episodes\n",
                "successes = 0\n",
                "total_steps = 0\n",
                "N = 50\n",
                "\n",
                "for i in range(N):\n",
                "    env = MiniGridFlatEnv({\"env_name\": \"BabyAI-GoToObj-v0\", \"max_steps\": 64})\n",
                "    obs, _ = env.reset(seed=3000+i)\n",
                "    done = False\n",
                "    steps = 0\n",
                "    \n",
                "    while not done:\n",
                "        action = algo.compute_single_action(obs)\n",
                "        obs, reward, term, trunc, _ = env.step(action)\n",
                "        done = term or trunc\n",
                "        steps += 1\n",
                "    \n",
                "    if term and reward > 0:\n",
                "        successes += 1\n",
                "    total_steps += steps\n",
                "\n",
                "print(f\"\\nüìä Results on {N} episodes:\")\n",
                "print(f\"   Success Rate: {successes/N:.1%}\")\n",
                "print(f\"   Avg Steps: {total_steps/N:.1f}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cleanup\n",
                "algo.stop()\n",
                "ray.shutdown()\n",
                "print(\"Done!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}