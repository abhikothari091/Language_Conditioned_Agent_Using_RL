{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸ¤– Language-Conditioned RL Agent Training\n",
                "\n",
                "This notebook trains a language-conditioned agent on BabyAI environments.\n",
                "\n",
                "## Setup Instructions\n",
                "\n",
                "1. **Runtime** â†’ **Change runtime type** â†’ Select **T4 GPU**\n",
                "2. Run all cells in order\n",
                "3. Download the trained model weights at the end\n",
                "\n",
                "**Expected Time:**\n",
                "- PPO Training (200 iterations): ~30-45 minutes\n",
                "- PPO Training (500 iterations): ~1-2 hours"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Install Dependencies"
            ],
            "metadata": {
                "id": "install_header"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install minigrid gymnasium numpy pandas tqdm -q\n",
                "!pip install torch -q\n",
                "!pip install 'ray[rllib]' -q\n",
                "\n",
                "print(\"\\nâœ… Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Create Environment Wrapper\n",
                "\n",
                "**Key Fix**: RLlib requires a simple `Box` observation space, not `Dict`. We flatten the 7x7x3 image + direction into a single vector."
            ],
            "metadata": {
                "id": "env_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Create RLlib-compatible environment with FLATTENED observation space\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "import numpy as np\n",
                "import minigrid\n",
                "\n",
                "class MiniGridRLlibEnv(gym.Env):\n",
                "    \"\"\"\n",
                "    MiniGrid wrapper with FLAT Box observation space for RLlib compatibility.\n",
                "    \n",
                "    Observation: Box(151,) = flattened 7x7x3 image (147) + one-hot direction (4)\n",
                "    Action: Discrete(7) = left, right, forward, pickup, drop, toggle, done\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, config=None):\n",
                "        super().__init__()\n",
                "        config = config or {}\n",
                "        env_name = config.get(\"env_name\", \"BabyAI-GoToObj-v0\")\n",
                "        max_steps = config.get(\"max_steps\", 64)\n",
                "        \n",
                "        # Create base environment\n",
                "        self.env = gym.make(env_name, render_mode=\"rgb_array\")\n",
                "        self.env.unwrapped.max_steps = max_steps\n",
                "        \n",
                "        # Flattened observation: 7*7*3 (image) + 4 (one-hot direction) = 151\n",
                "        self.observation_space = spaces.Box(\n",
                "            low=0.0, high=1.0, shape=(151,), dtype=np.float32\n",
                "        )\n",
                "        self.action_space = self.env.action_space  # Discrete(7)\n",
                "        \n",
                "        self._current_instruction = \"\"\n",
                "    \n",
                "    def _flatten_obs(self, obs):\n",
                "        \"\"\"Convert MiniGrid obs to flat vector.\"\"\"\n",
                "        # Normalize image to [0, 1]\n",
                "        image = obs[\"image\"].flatten().astype(np.float32) / 10.0  # Max value is ~10\n",
                "        \n",
                "        # One-hot encode direction (0-3)\n",
                "        direction = np.zeros(4, dtype=np.float32)\n",
                "        direction[obs[\"direction\"]] = 1.0\n",
                "        \n",
                "        return np.concatenate([image, direction])\n",
                "    \n",
                "    def reset(self, *, seed=None, options=None):\n",
                "        obs, info = self.env.reset(seed=seed, options=options)\n",
                "        self._current_instruction = self.env.unwrapped.mission\n",
                "        return self._flatten_obs(obs), info\n",
                "    \n",
                "    def step(self, action):\n",
                "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
                "        return self._flatten_obs(obs), reward, terminated, truncated, info\n",
                "    \n",
                "    def render(self):\n",
                "        return self.env.render()\n",
                "    \n",
                "    @property\n",
                "    def instruction(self):\n",
                "        return self._current_instruction\n",
                "\n",
                "# Test it\n",
                "env = MiniGridRLlibEnv()\n",
                "obs, info = env.reset(seed=42)\n",
                "print(f\"Observation space: {env.observation_space}\")\n",
                "print(f\"Observation shape: {obs.shape}\")\n",
                "print(f\"Action space: {env.action_space}\")\n",
                "print(f\"Instruction: {env.instruction}\")\n",
                "print(\"\\nâœ… Environment created!\")"
            ],
            "metadata": {
                "id": "create_env"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. Configure and Build PPO"
            ],
            "metadata": {
                "id": "ppo_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import ray\n",
                "from ray.rllib.algorithms.ppo import PPOConfig\n",
                "import torch\n",
                "\n",
                "# Initialize Ray\n",
                "ray.init(ignore_reinit_error=True)\n",
                "\n",
                "# Configure PPO with the flat observation environment\n",
                "config = (\n",
                "    PPOConfig()\n",
                "    .environment(\n",
                "        env=MiniGridRLlibEnv,\n",
                "        env_config={\n",
                "            \"env_name\": \"BabyAI-GoToObj-v0\",\n",
                "            \"max_steps\": 64,\n",
                "        },\n",
                "    )\n",
                "    .framework(\"torch\")\n",
                "    .env_runners(\n",
                "        num_env_runners=2,\n",
                "        num_envs_per_env_runner=4,\n",
                "    )\n",
                "    .training(\n",
                "        train_batch_size=2048,\n",
                "        lr=3e-4,\n",
                "        gamma=0.99,\n",
                "        clip_param=0.2,\n",
                "        num_sgd_iter=10,\n",
                "        entropy_coeff=0.01,\n",
                "        model={\n",
                "            \"fcnet_hiddens\": [256, 256],\n",
                "            \"fcnet_activation\": \"relu\",\n",
                "        },\n",
                "    )\n",
                "    .resources(num_gpus=1 if torch.cuda.is_available() else 0)\n",
                ")\n",
                "\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print(\"\\nâœ… PPO config created!\")"
            ],
            "metadata": {
                "id": "ppo_config"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Build the algorithm\n",
                "print(\"Building PPO algorithm...\")\n",
                "algo = config.build()\n",
                "print(\"âœ… PPO algorithm built!\")"
            ],
            "metadata": {
                "id": "build_algo"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. Train!"
            ],
            "metadata": {
                "id": "train_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "os.makedirs('experiments/checkpoints', exist_ok=True)\n",
                "\n",
                "# Training parameters\n",
                "NUM_ITERATIONS = 200  # Increase to 500+ for better results\n",
                "\n",
                "results = []\n",
                "best_reward = float('-inf')\n",
                "\n",
                "print(f\"Starting training for {NUM_ITERATIONS} iterations...\\n\")\n",
                "\n",
                "for i in range(NUM_ITERATIONS):\n",
                "    result = algo.train()\n",
                "    \n",
                "    # Extract metrics\n",
                "    reward_mean = result.get(\"env_runners\", {}).get(\"episode_reward_mean\", \n",
                "                   result.get(\"episode_reward_mean\", 0))\n",
                "    episode_len = result.get(\"env_runners\", {}).get(\"episode_len_mean\",\n",
                "                   result.get(\"episode_len_mean\", 0))\n",
                "    timesteps = result.get(\"timesteps_total\", 0)\n",
                "    \n",
                "    results.append({\n",
                "        \"iteration\": i + 1,\n",
                "        \"reward_mean\": reward_mean if reward_mean else 0,\n",
                "        \"episode_len\": episode_len if episode_len else 0,\n",
                "        \"timesteps\": timesteps,\n",
                "    })\n",
                "    \n",
                "    # Progress update every 10 iterations\n",
                "    if (i + 1) % 10 == 0:\n",
                "        print(f\"Iter {i+1:3d}/{NUM_ITERATIONS}: reward={reward_mean:6.2f}, len={episode_len:5.1f}, steps={timesteps}\")\n",
                "    \n",
                "    # Save checkpoint every 50 iterations\n",
                "    if (i + 1) % 50 == 0:\n",
                "        checkpoint = algo.save(\"experiments/checkpoints\")\n",
                "        print(f\"  ðŸ’¾ Checkpoint saved!\")\n",
                "    \n",
                "    # Track best\n",
                "    if reward_mean and reward_mean > best_reward:\n",
                "        best_reward = reward_mean\n",
                "\n",
                "print(f\"\\nâœ… Training complete!\")\n",
                "print(f\"Best reward: {best_reward:.2f}\")"
            ],
            "metadata": {
                "id": "train_loop"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5. Visualize Results"
            ],
            "metadata": {
                "id": "viz_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot training curve\n",
                "iterations = [r[\"iteration\"] for r in results]\n",
                "rewards = [r[\"reward_mean\"] for r in results]\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(iterations, rewards, 'b-', alpha=0.7)\n",
                "# Smoothed line\n",
                "if len(rewards) > 10:\n",
                "    smooth_rewards = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
                "    plt.plot(range(10, len(rewards)+1), smooth_rewards, 'r-', linewidth=2, label='Smoothed')\n",
                "plt.xlabel(\"Iteration\")\n",
                "plt.ylabel(\"Mean Episode Reward\")\n",
                "plt.title(\"PPO Training Curve\")\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.legend()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "ep_lens = [r[\"episode_len\"] for r in results]\n",
                "plt.plot(iterations, ep_lens, 'g-', alpha=0.7)\n",
                "plt.xlabel(\"Iteration\")\n",
                "plt.ylabel(\"Mean Episode Length\")\n",
                "plt.title(\"Episode Length (lower = faster completion)\")\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(\"training_curve.png\", dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nFinal reward: {rewards[-1]:.2f}\")\n",
                "print(f\"Final episode length: {ep_lens[-1]:.1f}\")"
            ],
            "metadata": {
                "id": "plot_curve"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6. Evaluate"
            ],
            "metadata": {
                "id": "eval_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Evaluate the trained agent\n",
                "from tqdm import tqdm\n",
                "\n",
                "def evaluate(algo, num_episodes=100):\n",
                "    env = MiniGridRLlibEnv({\"env_name\": \"BabyAI-GoToObj-v0\", \"max_steps\": 64})\n",
                "    successes = 0\n",
                "    total_reward = 0\n",
                "    total_steps = 0\n",
                "    \n",
                "    for ep in tqdm(range(num_episodes), desc=\"Evaluating\"):\n",
                "        obs, info = env.reset(seed=1000 + ep)  # Different seeds from training\n",
                "        done = False\n",
                "        ep_reward = 0\n",
                "        steps = 0\n",
                "        \n",
                "        while not done:\n",
                "            action = algo.compute_single_action(obs)\n",
                "            obs, reward, terminated, truncated, info = env.step(action)\n",
                "            done = terminated or truncated\n",
                "            ep_reward += reward\n",
                "            steps += 1\n",
                "        \n",
                "        if terminated and ep_reward > 0:\n",
                "            successes += 1\n",
                "        total_reward += ep_reward\n",
                "        total_steps += steps\n",
                "    \n",
                "    return {\n",
                "        \"success_rate\": successes / num_episodes,\n",
                "        \"mean_reward\": total_reward / num_episodes,\n",
                "        \"mean_steps\": total_steps / num_episodes,\n",
                "    }\n",
                "\n",
                "print(\"\\nðŸ“Š Evaluating trained agent...\\n\")\n",
                "eval_results = evaluate(algo, num_episodes=100)\n",
                "\n",
                "print(f\"\\n{'='*40}\")\n",
                "print(f\"ðŸ“Š EVALUATION RESULTS\")\n",
                "print(f\"{'='*40}\")\n",
                "print(f\"Success Rate: {eval_results['success_rate']:.1%}\")\n",
                "print(f\"Mean Reward:  {eval_results['mean_reward']:.3f}\")\n",
                "print(f\"Mean Steps:   {eval_results['mean_steps']:.1f}\")\n",
                "print(f\"{'='*40}\")"
            ],
            "metadata": {
                "id": "evaluate"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 7. Save & Download"
            ],
            "metadata": {
                "id": "save_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Save final checkpoint\n",
                "import json\n",
                "\n",
                "final_checkpoint = algo.save(\"experiments/checkpoints/final\")\n",
                "print(f\"ðŸ’¾ Final checkpoint saved: {final_checkpoint}\")\n",
                "\n",
                "# Save evaluation results\n",
                "with open(\"eval_results.json\", \"w\") as f:\n",
                "    json.dump(eval_results, f, indent=2)\n",
                "\n",
                "# Save training results\n",
                "with open(\"training_results.json\", \"w\") as f:\n",
                "    json.dump(results, f, indent=2)\n",
                "\n",
                "# Zip for download\n",
                "!zip -r trained_model.zip experiments/checkpoints/ eval_results.json training_results.json\n",
                "print(\"\\nðŸ“¦ Model zipped: trained_model.zip\")"
            ],
            "metadata": {
                "id": "save_model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Download files\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Downloading files...\")\n",
                "files.download('trained_model.zip')\n",
                "files.download('training_curve.png')\n",
                "print(\"\\nâœ… Download complete!\")"
            ],
            "metadata": {
                "id": "download"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cleanup\n",
                "algo.stop()\n",
                "ray.shutdown()\n",
                "print(\"\\nâœ… Done! Your trained model is in trained_model.zip\")"
            ],
            "metadata": {
                "id": "cleanup"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}