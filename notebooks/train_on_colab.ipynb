{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸ¤– Language-Conditioned RL Agent Training\n",
                "\n",
                "This notebook trains a language-conditioned agent on BabyAI environments.\n",
                "\n",
                "## Setup Instructions\n",
                "\n",
                "1. **Runtime** â†’ **Change runtime type** â†’ Select **T4 GPU**\n",
                "2. Run all cells in order\n",
                "3. Download the trained model weights at the end\n",
                "\n",
                "**Expected Time:**\n",
                "- BC Training: ~5 minutes\n",
                "- PPO Training (500 iterations): ~1-2 hours"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. Install Dependencies"
            ],
            "metadata": {
                "id": "install_header"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install minigrid gymnasium numpy pandas jsonlines pyyaml tqdm python-dotenv -q\n",
                "!pip install torch transformers accelerate huggingface_hub -q\n",
                "!pip install 'ray[rllib]' -q\n",
                "\n",
                "print(\"\\nâœ… Dependencies installed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. Clone Repository or Upload Code"
            ],
            "metadata": {
                "id": "clone_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Option A: Clone from GitHub (if you pushed your code)\n",
                "# !git clone https://github.com/YOUR_USERNAME/Language_Conditioned_Agent_Using_RL.git\n",
                "# %cd Language_Conditioned_Agent_Using_RL\n",
                "\n",
                "# Option B: Upload code manually\n",
                "# Use the file browser on the left to upload your 'src' folder\n",
                "\n",
                "# For now, let's create the minimal code needed for training:\n",
                "import os\n",
                "os.makedirs('src/environment', exist_ok=True)\n",
                "os.makedirs('src/agents/planner', exist_ok=True)\n",
                "os.makedirs('src/agents/executor', exist_ok=True)\n",
                "os.makedirs('src/training', exist_ok=True)\n",
                "os.makedirs('src/evaluation', exist_ok=True)\n",
                "os.makedirs('data/demos', exist_ok=True)\n",
                "os.makedirs('experiments/checkpoints', exist_ok=True)\n",
                "\n",
                "print(\"ðŸ“ Directory structure created!\")"
            ],
            "metadata": {
                "id": "clone_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Create the minimal environment wrapper\n",
                "env_wrapper_code = '''\n",
                "\"\"\"MiniGrid wrapper for training.\"\"\"\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "import numpy as np\n",
                "\n",
                "class MiniGridWrapper(gym.Wrapper):\n",
                "    ACTION_NAMES = [\"left\", \"right\", \"forward\", \"pickup\", \"drop\", \"toggle\", \"done\"]\n",
                "\n",
                "    def __init__(self, env_name=\"BabyAI-GoToObj-v0\", max_steps=64, use_dense_reward=True, seed=None):\n",
                "        import minigrid\n",
                "        base_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
                "        if max_steps:\n",
                "            base_env.unwrapped.max_steps = max_steps\n",
                "        super().__init__(base_env)\n",
                "        self.use_dense_reward = use_dense_reward\n",
                "        self._seed = seed\n",
                "        self._current_instruction = \"\"\n",
                "        self._prev_distance = None\n",
                "\n",
                "        self.observation_space = spaces.Dict({\n",
                "            \"image\": self.env.observation_space[\"image\"],\n",
                "            \"direction\": spaces.Discrete(4),\n",
                "            \"instruction\": spaces.Text(max_length=256),\n",
                "        })\n",
                "\n",
                "    def reset(self, seed=None, options=None):\n",
                "        reset_seed = seed if seed is not None else self._seed\n",
                "        obs, info = self.env.reset(seed=reset_seed, options=options)\n",
                "        self._current_instruction = self.env.unwrapped.mission\n",
                "        self._prev_distance = None\n",
                "        return {\"image\": obs[\"image\"], \"direction\": obs[\"direction\"], \"instruction\": self._current_instruction}, info\n",
                "\n",
                "    def step(self, action):\n",
                "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
                "        if self.use_dense_reward:\n",
                "            reward = self._shape_reward(reward, terminated)\n",
                "        return {\"image\": obs[\"image\"], \"direction\": obs[\"direction\"], \"instruction\": self._current_instruction}, reward, terminated, truncated, info\n",
                "\n",
                "    def _shape_reward(self, reward, terminated):\n",
                "        if not terminated and hasattr(self.env.unwrapped, \"agent_pos\"):\n",
                "            try:\n",
                "                pos = self.env.unwrapped.agent_pos\n",
                "                goal = getattr(self.env.unwrapped, \"goal_pos\", None)\n",
                "                if goal:\n",
                "                    dist = abs(pos[0]-goal[0]) + abs(pos[1]-goal[1])\n",
                "                    if self._prev_distance:\n",
                "                        reward += 0.01 * (self._prev_distance - dist)\n",
                "                    self._prev_distance = dist\n",
                "            except:\n",
                "                pass\n",
                "        return reward\n",
                "\n",
                "    @property\n",
                "    def instruction(self):\n",
                "        return self._current_instruction\n",
                "\n",
                "def make_env(env_name=\"BabyAI-GoToObj-v0\", **kwargs):\n",
                "    return MiniGridWrapper(env_name, **kwargs)\n",
                "'''\n",
                "\n",
                "with open('src/environment/__init__.py', 'w') as f:\n",
                "    f.write('from src.environment.minigrid_wrapper import MiniGridWrapper, make_env\\n')\n",
                "\n",
                "with open('src/environment/minigrid_wrapper.py', 'w') as f:\n",
                "    f.write(env_wrapper_code)\n",
                "\n",
                "print(\"âœ… Environment wrapper created!\")"
            ],
            "metadata": {
                "id": "create_env_wrapper"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Create RLlib environment wrapper\n",
                "rllib_env_code = '''\n",
                "\"\"\"RLlib environment wrapper.\"\"\"\n",
                "import gymnasium as gym\n",
                "from gymnasium import spaces\n",
                "import numpy as np\n",
                "\n",
                "class MiniGridRLlibEnv(gym.Env):\n",
                "    def __init__(self, config):\n",
                "        from src.environment.minigrid_wrapper import MiniGridWrapper\n",
                "        env_name = config.get(\"env_name\", \"BabyAI-GoToObj-v0\")\n",
                "        max_steps = config.get(\"max_steps\", 64)\n",
                "        use_dense_reward = config.get(\"use_dense_reward\", True)\n",
                "        self.env = MiniGridWrapper(env_name=env_name, max_steps=max_steps, use_dense_reward=use_dense_reward)\n",
                "\n",
                "        self.observation_space = spaces.Dict({\n",
                "            \"image\": spaces.Box(low=0, high=255, shape=(7, 7, 3), dtype=np.float32),\n",
                "            \"direction\": spaces.Discrete(4),\n",
                "            \"subgoal\": spaces.Box(low=0, high=10, shape=(3,), dtype=np.int32),\n",
                "        })\n",
                "        self.action_space = self.env.action_space\n",
                "        self.current_subgoal = np.array([0, 0, 0], dtype=np.int32)\n",
                "\n",
                "    def reset(self, *, seed=None, options=None):\n",
                "        obs, info = self.env.reset(seed=seed, options=options)\n",
                "        return {\"image\": obs[\"image\"].astype(np.float32), \"direction\": obs[\"direction\"], \"subgoal\": self.current_subgoal}, info\n",
                "\n",
                "    def step(self, action):\n",
                "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
                "        return {\"image\": obs[\"image\"].astype(np.float32), \"direction\": obs[\"direction\"], \"subgoal\": self.current_subgoal}, reward, terminated, truncated, info\n",
                "\n",
                "    def render(self):\n",
                "        return self.env.render()\n",
                "'''\n",
                "\n",
                "with open('src/agents/__init__.py', 'w') as f:\n",
                "    f.write('')\n",
                "with open('src/agents/executor/__init__.py', 'w') as f:\n",
                "    f.write('from src.agents.executor.rllib_env import MiniGridRLlibEnv\\n')\n",
                "with open('src/agents/executor/rllib_env.py', 'w') as f:\n",
                "    f.write(rllib_env_code)\n",
                "\n",
                "print(\"âœ… RLlib environment wrapper created!\")"
            ],
            "metadata": {
                "id": "create_rllib_env"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. Verify Setup"
            ],
            "metadata": {
                "id": "verify_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import sys\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "\n",
                "# Test environment\n",
                "from src.environment.minigrid_wrapper import MiniGridWrapper\n",
                "env = MiniGridWrapper(\"BabyAI-GoToObj-v0\")\n",
                "obs, info = env.reset(seed=42)\n",
                "print(f\"\\nEnvironment test:\")\n",
                "print(f\"  Instruction: {obs['instruction']}\")\n",
                "print(f\"  Image shape: {obs['image'].shape}\")\n",
                "\n",
                "print(\"\\nâœ… Setup verified!\")"
            ],
            "metadata": {
                "id": "verify_setup"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. Generate Expert Demonstrations (Optional BC)"
            ],
            "metadata": {
                "id": "bc_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Generate demonstrations using BabyAI bot\n",
                "import gymnasium as gym\n",
                "import minigrid\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "\n",
                "def generate_demos(env_name=\"BabyAI-GoToObj-v0\", num_episodes=500, seed=42):\n",
                "    \"\"\"Generate expert demonstrations.\"\"\"\n",
                "    try:\n",
                "        from minigrid.utils.baby_ai_bot import BabyAIBot\n",
                "        has_bot = True\n",
                "    except ImportError:\n",
                "        has_bot = False\n",
                "        print(\"Warning: BabyAI bot not available, using heuristic policy\")\n",
                "\n",
                "    env = gym.make(env_name)\n",
                "    demos = []\n",
                "    np.random.seed(seed)\n",
                "\n",
                "    for ep in tqdm(range(num_episodes), desc=\"Generating demos\"):\n",
                "        obs, info = env.reset(seed=seed + ep)\n",
                "        episode_data = {\"obs\": [], \"actions\": [], \"instruction\": env.unwrapped.mission}\n",
                "\n",
                "        if has_bot:\n",
                "            bot = BabyAIBot(env.unwrapped)\n",
                "\n",
                "        done = False\n",
                "        while not done:\n",
                "            episode_data[\"obs\"].append(obs[\"image\"].copy())\n",
                "\n",
                "            if has_bot:\n",
                "                try:\n",
                "                    action = bot.replan()\n",
                "                    if action is None:\n",
                "                        action = env.action_space.sample()\n",
                "                except:\n",
                "                    action = env.action_space.sample()\n",
                "            else:\n",
                "                action = env.action_space.sample()\n",
                "\n",
                "            episode_data[\"actions\"].append(action)\n",
                "            obs, reward, terminated, truncated, info = env.step(action)\n",
                "            done = terminated or truncated\n",
                "\n",
                "        episode_data[\"success\"] = terminated and reward > 0\n",
                "        if episode_data[\"success\"]:\n",
                "            demos.append(episode_data)\n",
                "\n",
                "    env.close()\n",
                "    print(f\"Generated {len(demos)} successful demonstrations\")\n",
                "    return demos\n",
                "\n",
                "# Generate demos\n",
                "demos = generate_demos(num_episodes=500)\n",
                "print(f\"\\nâœ… Generated {len(demos)} demos!\")"
            ],
            "metadata": {
                "id": "generate_demos"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 5. PPO Training"
            ],
            "metadata": {
                "id": "ppo_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import ray\n",
                "from ray.rllib.algorithms.ppo import PPOConfig\n",
                "from src.agents.executor.rllib_env import MiniGridRLlibEnv\n",
                "\n",
                "# Initialize Ray\n",
                "ray.init(ignore_reinit_error=True)\n",
                "\n",
                "# Configure PPO\n",
                "config = (\n",
                "    PPOConfig()\n",
                "    .environment(\n",
                "        env=MiniGridRLlibEnv,\n",
                "        env_config={\n",
                "            \"env_name\": \"BabyAI-GoToObj-v0\",\n",
                "            \"max_steps\": 64,\n",
                "            \"use_dense_reward\": True,\n",
                "        },\n",
                "    )\n",
                "    .framework(\"torch\")\n",
                "    .env_runners(\n",
                "        num_env_runners=2,\n",
                "        num_envs_per_env_runner=4,\n",
                "    )\n",
                "    .training(\n",
                "        train_batch_size=2048,\n",
                "        lr=3e-4,\n",
                "        gamma=0.99,\n",
                "        clip_param=0.2,\n",
                "        num_sgd_iter=10,\n",
                "        entropy_coeff=0.01,\n",
                "    )\n",
                "    .resources(num_gpus=1 if torch.cuda.is_available() else 0)\n",
                ")\n",
                "\n",
                "print(\"âœ… PPO config created!\")"
            ],
            "metadata": {
                "id": "ppo_config"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Build and train\n",
                "algo = config.build()\n",
                "print(\"âœ… PPO algorithm built!\")\n",
                "\n",
                "# Training loop\n",
                "NUM_ITERATIONS = 200  # Increase for better results (500+ recommended)\n",
                "\n",
                "results = []\n",
                "for i in range(NUM_ITERATIONS):\n",
                "    result = algo.train()\n",
                "\n",
                "    # Log progress\n",
                "    reward_mean = result.get(\"episode_reward_mean\", 0)\n",
                "    episode_len = result.get(\"episode_len_mean\", 0)\n",
                "    timesteps = result.get(\"timesteps_total\", 0)\n",
                "\n",
                "    results.append({\n",
                "        \"iteration\": i + 1,\n",
                "        \"reward_mean\": reward_mean,\n",
                "        \"episode_len\": episode_len,\n",
                "        \"timesteps\": timesteps,\n",
                "    })\n",
                "\n",
                "    if (i + 1) % 10 == 0:\n",
                "        print(f\"Iter {i+1}/{NUM_ITERATIONS}: reward={reward_mean:.2f}, len={episode_len:.1f}, timesteps={timesteps}\")\n",
                "\n",
                "    # Save checkpoint every 50 iterations\n",
                "    if (i + 1) % 50 == 0:\n",
                "        checkpoint = algo.save(\"experiments/checkpoints\")\n",
                "        print(f\"  ðŸ’¾ Checkpoint saved: {checkpoint}\")\n",
                "\n",
                "print(\"\\nâœ… Training complete!\")"
            ],
            "metadata": {
                "id": "ppo_train"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Plot training curve\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "iterations = [r[\"iteration\"] for r in results]\n",
                "rewards = [r[\"reward_mean\"] for r in results]\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(iterations, rewards)\n",
                "plt.xlabel(\"Iteration\")\n",
                "plt.ylabel(\"Mean Episode Reward\")\n",
                "plt.title(\"PPO Training Curve\")\n",
                "plt.grid(True)\n",
                "plt.savefig(\"training_curve.png\")\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nFinal reward: {rewards[-1]:.2f}\")"
            ],
            "metadata": {
                "id": "plot_curve"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 6. Evaluate and Save"
            ],
            "metadata": {
                "id": "eval_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Evaluate the trained agent\n",
                "def evaluate(algo, num_episodes=100):\n",
                "    env = MiniGridRLlibEnv({\"env_name\": \"BabyAI-GoToObj-v0\", \"max_steps\": 64})\n",
                "    successes = 0\n",
                "    total_reward = 0\n",
                "\n",
                "    for ep in range(num_episodes):\n",
                "        obs, info = env.reset(seed=ep)\n",
                "        done = False\n",
                "        ep_reward = 0\n",
                "\n",
                "        while not done:\n",
                "            action = algo.compute_single_action(obs)\n",
                "            obs, reward, terminated, truncated, info = env.step(action)\n",
                "            done = terminated or truncated\n",
                "            ep_reward += reward\n",
                "\n",
                "        if terminated and reward > 0:\n",
                "            successes += 1\n",
                "        total_reward += ep_reward\n",
                "\n",
                "    return {\n",
                "        \"success_rate\": successes / num_episodes,\n",
                "        \"mean_reward\": total_reward / num_episodes,\n",
                "    }\n",
                "\n",
                "eval_results = evaluate(algo, num_episodes=100)\n",
                "print(f\"\\nðŸ“Š Evaluation Results:\")\n",
                "print(f\"   Success Rate: {eval_results['success_rate']:.1%}\")\n",
                "print(f\"   Mean Reward:  {eval_results['mean_reward']:.3f}\")"
            ],
            "metadata": {
                "id": "evaluate"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Save final checkpoint\n",
                "final_checkpoint = algo.save(\"experiments/checkpoints/final\")\n",
                "print(f\"\\nðŸ’¾ Final checkpoint saved: {final_checkpoint}\")\n",
                "\n",
                "# Zip for download\n",
                "!zip -r trained_model.zip experiments/checkpoints/\n",
                "print(\"\\nðŸ“¦ Model zipped: trained_model.zip\")\n",
                "print(\"\\nDownload this file and extract to your local project!\")"
            ],
            "metadata": {
                "id": "save_model"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Download the trained model\n",
                "from google.colab import files\n",
                "files.download('trained_model.zip')\n",
                "files.download('training_curve.png')"
            ],
            "metadata": {
                "id": "download"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Cleanup\n",
                "algo.stop()\n",
                "ray.shutdown()\n",
                "print(\"\\nâœ… Done! Download your trained model and training curve.\")"
            ],
            "metadata": {
                "id": "cleanup"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}